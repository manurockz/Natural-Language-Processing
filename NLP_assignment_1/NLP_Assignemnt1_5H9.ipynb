{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignemnt1_5H9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdxzRwti5wqp"
      },
      "source": [
        "##**NLP ASSIGNMENT 1**\n",
        "##**- *18K41A05H9*** \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd5WH4CDJF5u",
        "outputId": "d28bc635-fd38-41d4-bd72-8afe4b28be88"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrMd0jBdJJZg"
      },
      "source": [
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI1uSPWCKVxa",
        "outputId": "d65f25f0-0f02-4294-e764-76bdb9455ec2"
      },
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1U4ESUyJYb9",
        "outputId": "7e64d886-866a-4839-9b7f-44d3ed9dab75"
      },
      "source": [
        "stops=list(stopwords.words(\"english\"))\n",
        "print(len(stops))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ksch3_rQJbFa"
      },
      "source": [
        "paragraph = \"Are  you  fascinated  by  the  amount  of  text  data  available  on  the  internet?  Are  you looking  for  ways  to  work  with  this  text  data  but  aren’t  sure  where  to  begin? Machines, after all, recognize numbers, not the letters of our language. And that can be a tricky landscape to navigate in machine learning.\"\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNTjADhNKuwZ"
      },
      "source": [
        "#**1.splitting above paragraph into sentences Using sentence Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVyI9amnKsyR"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM4vgnxbMRU2",
        "outputId": "11553fba-5c43-48fb-ef10-0f0e3fadb4c5"
      },
      "source": [
        "sentences=sent_tokenize(paragraph)\n",
        "print(sentences)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Are  you  fascinated  by  the  amount  of  text  data  available  on  the  internet?', 'Are  you looking  for  ways  to  work  with  this  text  data  but  aren’t  sure  where  to  begin?', 'Machines, after all, recognize numbers, not the letters of our language.', 'And that can be a tricky landscape to navigate in machine learning.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHNL6NLwMXBs",
        "outputId": "78686ab2-a00e-41b8-b6a5-c62b745ae850"
      },
      "source": [
        "print(\"The paragraph has total \"+str(len(sentences))+\" sentences. And those are:\\n\")\n",
        "for i in range(len(sentences)):\n",
        "  print(\"Sentence \"+str(i+1)+\" : \"+sentences[i])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The paragraph has total 4 sentences. And those are:\n",
            "\n",
            "Sentence 1 : Are  you  fascinated  by  the  amount  of  text  data  available  on  the  internet?\n",
            "Sentence 2 : Are  you looking  for  ways  to  work  with  this  text  data  but  aren’t  sure  where  to  begin?\n",
            "Sentence 3 : Machines, after all, recognize numbers, not the letters of our language.\n",
            "Sentence 4 : And that can be a tricky landscape to navigate in machine learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lDYQycINsxC"
      },
      "source": [
        "##**2.splitting above sentence into words using word tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xQx3js2Nr7o"
      },
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BblZ491LM2BJ",
        "outputId": "ce8b4b98-d331-4749-d5b3-0130f7b91999"
      },
      "source": [
        "words=word_tokenize(paragraph)\n",
        "print(words)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Are', 'you', 'fascinated', 'by', 'the', 'amount', 'of', 'text', 'data', 'available', 'on', 'the', 'internet', '?', 'Are', 'you', 'looking', 'for', 'ways', 'to', 'work', 'with', 'this', 'text', 'data', 'but', 'aren', '’', 't', 'sure', 'where', 'to', 'begin', '?', 'Machines', ',', 'after', 'all', ',', 'recognize', 'numbers', ',', 'not', 'the', 'letters', 'of', 'our', 'language', '.', 'And', 'that', 'can', 'be', 'a', 'tricky', 'landscape', 'to', 'navigate', 'in', 'machine', 'learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_5H6GfIN9LX",
        "outputId": "e173aaf7-31a7-489b-e748-4be55a6b6ea4"
      },
      "source": [
        "#word_tokenize will assume punctuation marks as separate words\n",
        "print(\"Total number of words encountered in paragraph =\",len(words))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of words encountered in paragraph = 62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo8uwG8SuYfp"
      },
      "source": [
        "##**3.finding stema and lemma for given words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J5t06J-uVAv"
      },
      "source": [
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
        "stemmer=LancasterStemmer()\n",
        "lemmetizer=WordNetLemmatizer()"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yih4d75CORaC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e39e50-9b92-4a0c-9aee-512cd22c4c13"
      },
      "source": [
        "#word1:\"cats\"\n",
        "print(\"Stemmed word of cats: \",stemmer.stem('cats'))\n",
        "print(\"Lemmatized word of cats: \",lemmetizer.lemmatize('cats'))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed word of cats:  cat\n",
            "Lemmatized word of cats:  cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv3zmsnhu_4I",
        "outputId": "35752404-a5f2-4b32-f3e9-404ada8d30c3"
      },
      "source": [
        "#word2:\"trouble\"\n",
        "print(\"Stemmed word of trouble: \",stemmer.stem('trouble'))\n",
        "print(\"Lemmatized word of trouble: \",lemmetizer.lemmatize('trouble'))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed word of trouble:  troubl\n",
            "Lemmatized word of trouble:  trouble\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLko6IQ1vNjV",
        "outputId": "5ad948db-1574-4b1a-e152-32f324b27fa1"
      },
      "source": [
        "#word3:\"troubling\"\n",
        "print(\"Stemmed word of troubling: \",stemmer.stem('troubling'))\n",
        "print(\"Lemmatized word of troubling: \",lemmetizer.lemmatize('troubling'))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed word of troubling:  troubl\n",
            "Lemmatized word of troubling:  troubling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo-Hw38tvdrH",
        "outputId": "b6ca3e2b-5ab6-4d59-badf-1eec70738157"
      },
      "source": [
        "#word4:\"troubled\"\n",
        "print(\"Stemmed word of troubled: \",stemmer.stem('troubled'))\n",
        "print(\"Lemmatized word of troubled: \",lemmetizer.lemmatize('troubled'))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed word of troubled:  troubl\n",
            "Lemmatized word of troubled:  troubled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZrNjQsJvig-",
        "outputId": "1c372b49-6b8b-4922-fb4b-c1e3f853af81"
      },
      "source": [
        "#word5:\"having\"\n",
        "print(\"Stemmed word of having: \",stemmer.stem('having'))\n",
        "print(\"Lemmatized word of having: \",lemmetizer.lemmatize('having'))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed word of having:  hav\n",
            "Lemmatized word of having:  having\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5-dQKiOvoWg",
        "outputId": "bc111d0c-83f8-4dc7-bd7c-183f71f186fb"
      },
      "source": [
        "#word6:\"Corriendo\"\n",
        "print(\"Stemmed word of corriendo: \",stemmer.stem('corriendo'))\n",
        "print(\"Lemmatized word of corriendo: \",lemmetizer.lemmatize('corriendo'))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed word of corriendo:  corriendo\n",
            "Lemmatized word of corriendo:  corriendo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpHqxxRPvtUX",
        "outputId": "fc860e40-8fa7-4821-b94d-970123cffa95"
      },
      "source": [
        "#word7:\"at\"\n",
        "print(\"Stemmed word of at: \",stemmer.stem('at'))\n",
        "print(\"Lemmatized word of at: \",lemmetizer.lemmatize('at'))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed word of at:  at\n",
            "Lemmatized word of at:  at\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b_PgDQ4vw6X",
        "outputId": "ee76fbe9-ecf7-486e-b7b6-f72193bb3a42"
      },
      "source": [
        "#word8:\"was\"\n",
        "print(\"Stemmed word of was: \",stemmer.stem('was'))\n",
        "print(\"Lemmatized word of was: \",lemmetizer.lemmatize('was'))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed word of was:  was\n",
            "Lemmatized word of was:  wa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tqs-YJLyW-1"
      },
      "source": [
        "##**4.find stop words from the given paragraph**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeBromkSv2O1"
      },
      "source": [
        "paragraph2=\"The NLTK library  is  one  of  the  oldest  and  most  commonly  used  Python  libraries  for Natural Language Processing. NLTK supports stop word removal, and you can find the list of stop words in the  corpus  module. To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK.\""
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gICNXJtyqfg",
        "outputId": "30acf32b-e17f-4f4e-c84a-a8680493f1a0"
      },
      "source": [
        "triggered=[] #storing encountered stopwords in this list\n",
        "p1=word_tokenize(paragraph2) #splitting words of paragraph\n",
        "for i in p1:\n",
        "  if i in stopwords.words():\n",
        "    triggered.append(i)\n",
        "\n",
        "print(triggered)\n",
        "print(\"encountered\",len(triggered))\n",
        "print(len(set(triggered)),\"unique stopwords found in given paragraph\")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['is', 'one', 'of', 'the', 'and', 'most', 'for', 'and', 'you', 'can', 'the', 'of', 'in', 'the', 'from', 'a', 'you', 'can', 'your', 'into', 'and', 'then', 'the', 'if', 'it', 'in', 'the', 'of', 'by']\n",
            "encountered 29\n",
            "18 unique stopwords found in given paragraph\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RmXIv0h1Px-"
      },
      "source": [
        "##**5.counting frequency of each word**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e884kJ3t0jF9",
        "outputId": "b4719a6c-278d-4501-bc50-908a38603d9c"
      },
      "source": [
        "from nltk import FreqDist\n",
        "frequency=FreqDist(p1) #module for calculating frequency in nltk\n",
        "counts=list(frequency.keys()) #keys are the unique words found in paragraph\n",
        "for i in counts:\n",
        "  print(i,\"--\",frequency[i])"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The -- 1\n",
            "NLTK -- 3\n",
            "library -- 1\n",
            "is -- 1\n",
            "one -- 1\n",
            "of -- 3\n",
            "the -- 5\n",
            "oldest -- 1\n",
            "and -- 3\n",
            "most -- 1\n",
            "commonly -- 1\n",
            "used -- 1\n",
            "Python -- 1\n",
            "libraries -- 1\n",
            "for -- 1\n",
            "Natural -- 1\n",
            "Language -- 1\n",
            "Processing -- 1\n",
            ". -- 3\n",
            "supports -- 1\n",
            "stop -- 4\n",
            "word -- 2\n",
            "removal -- 1\n",
            ", -- 2\n",
            "you -- 2\n",
            "can -- 2\n",
            "find -- 1\n",
            "list -- 2\n",
            "words -- 4\n",
            "in -- 2\n",
            "corpus -- 1\n",
            "module -- 1\n",
            "To -- 1\n",
            "remove -- 2\n",
            "from -- 1\n",
            "a -- 1\n",
            "sentence -- 1\n",
            "divide -- 1\n",
            "your -- 1\n",
            "text -- 1\n",
            "into -- 1\n",
            "then -- 1\n",
            "if -- 1\n",
            "it -- 1\n",
            "exits -- 1\n",
            "provided -- 1\n",
            "by -- 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q24ocEza3PJC",
        "outputId": "4681cfe1-c7e9-48b0-cf28-cb633f452834"
      },
      "source": [
        "#most frequent words i.e words whose freqeuncy>1.\n",
        "for i in counts:\n",
        "  if frequency[i]>1:\n",
        "    print(i,\"---\",frequency[i])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK --- 3\n",
            "of --- 3\n",
            "the --- 5\n",
            "and --- 3\n",
            ". --- 3\n",
            "stop --- 4\n",
            "word --- 2\n",
            ", --- 2\n",
            "you --- 2\n",
            "can --- 2\n",
            "list --- 2\n",
            "words --- 4\n",
            "in --- 2\n",
            "remove --- 2\n"
          ]
        }
      ]
    }
  ]
}